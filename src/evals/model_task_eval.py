"""
This file will evaluate whether a model can correctly identify that a sequence has been generated by some
class of function.
"""
import random

from evals.question_eval import choose_function
from evals.wrong_int_func_generator import generate_wrong_functions
from pipelines.sequence_completions import sequence_functions as all_sequence_functions


def model_task_evaluation(
    function_class: str,
    sequence_functions: dict[str, str],
    sequence_length: int,
    prompt_file: str,
    model_name: str,
    temperature: float = 0.0,
    num_questions: int = 50,
    num_functions: int = 5,
) -> int:
    """
    Generate questions about which function generated a sequence, and evaluate the model's performance.
    """
    correct = 0
    incorrect = 0
    invalid = 0
    fn_form = sequence_functions[function_class]
    with open(prompt_file) as f:
        prompt = f.read()
        for i in range(num_questions):
            print("Question: ", i + 1, "/", num_questions, sep="")
            # Generate a function from the class
            target_fn = fn_form.format(random.randint(0, 10), random.randint(0, 10))
            offset = random.randint(0, 10)
            # Generate a sequence
            target_sequence = [
                eval(target_fn)(j + offset) for j in range(sequence_length)
            ]

            # Generate incorrect functions
            incorrect_functions = generate_wrong_functions(
                target_sequence, num_functions
            )

            all_functions = incorrect_functions + [target_fn]

            # Load the prompt
            # Prompt the model to choose the correct function
            model_response = choose_function(
                possible_functions=all_functions,
                correct_function_indices=[num_functions + 1],
                target_sequence=target_sequence,
                prompt=prompt,
                model_name=model_name,
                temperature=temperature,
            )
            if model_response == 1:
                correct += 1
            elif model_response == 0:
                incorrect += 1
            elif model_response < 0:
                invalid += 1
    return correct, incorrect, invalid


if __name__ == "__main__":
    correct, incorrect, invalid = model_task_evaluation(
        function_class="modular_progression",
        sequence_functions=all_sequence_functions,
        sequence_length=5,
        prompt_file="evals/prompts/choose_function.txt",
        model_name="DAVINCI",
        temperature=0.0,
        num_questions=20,
        num_functions=5,
    )
    print(f"Correct: {correct}")
    print(f"Incorrect: {incorrect}")
    print(f"Invalid: {invalid}")
