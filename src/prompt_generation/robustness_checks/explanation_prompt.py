"""
Create a prompt to continue a sequence of numbers, in an arbitrary base.

Prompts will take the form:
--------------------------
    Here are some examples of sequence explanations, i.e. python functions
    which could have generated the preceding sequences. Assume the first
    number was generated by f(0), the second by f(1), and so on.
    Sequence: 2, 4, 6
    Explanation: lambda x: 2*(x+1)

    Sequence: 1, 2, 3, 4, 5
    Explanation: lambda x: (x+1)

    Sequence: 9, 16, 25, 36
    Explanation: lambda x: (x+3)**2

    ***EXPLANATION_PROMPT***

    Explanation:

"""


import random
from logging import getLogger
from typing import List, Optional, Union

from src.models.openai_model import (
    DAVINCI_MODEL_NAME,
    OpenAIChatModels,
    OpenAITextModels,
)
from src.pipelines import ShotSamplingType
from src.pipelines.sequence_completions import sequence_functions
from src.prompt_generation.robustness_checks.distribution_prompt import TASK_PROMPTS
from src.prompt_generation.robustness_checks.utils import (
    extend_prompt,
    generate_random_fn_sequence,
    initialise_prompt,
    start_question,
)

logger = getLogger(__name__)

PRE_PROMPT = """
Here are some examples of sequence explanations, i.e. python functions
which generated the preceding sequences base {}. Assume the first number was generated by f(0),
the second by f(1), the third by f(2), and so on.
"""

# TODO: fix generating functions to include recursive progressions, an ok fix for now.
sequence_functions = sequence_functions.copy()


def create_explanation_prompt(
    sequence: List[int],
    task_prompt: str,
    model_name: str = DAVINCI_MODEL_NAME,
    base: int = 10,
    shots: int = 0,
    shot_method: ShotSamplingType = ShotSamplingType.RANDOM,
    role_prompt: Optional[str] = None,
    seed: int = 0,
) -> Union[str, List[dict]]:
    """
    Create a prompt to continue a sequence of numbers.
    """
    random.seed(seed)
    sequence_length = len(sequence)
    prompt_text = initialise_prompt(model_name)
    # Generate the few shot examples
    if shots > 0:
        for _ in range(shots):
            # Note: we are using the sequence length implicitly specified by
            # the target sequence to generate the prompts.
            shot_prompt = generate_exp_shot_prompt(
                shot_method, sequence_length, model_name, base
            )
            prompt_text = extend_prompt(prompt_text, shot_prompt)

    # Generate the explanation prompt
    text = TASK_PROMPTS[task_prompt]["explanation"]
    text = start_question(text, sequence, base, role_prompt)
    pre_prompt = PRE_PROMPT
    pre_prompt = pre_prompt.format(base)
    # Combine together to form the final prompt
    if model_name in OpenAITextModels.list():
        assert isinstance(prompt_text, str)
        # Prepend to the shots
        pretext = pre_prompt + "\n"
        pretext += "\n"
        text = pretext + prompt_text + text
        text += "\n"
        text += "A: "
        return text
    elif model_name in OpenAIChatModels.list():
        assert isinstance(prompt_text, list)
        pretext = [
            {
                "role": "system",
                "content": pre_prompt,
            }
        ]
        whole_prompt = (
            pretext
            + prompt_text
            + [{"role": "user", "content": text}]
            + [{"role": "assistant", "content": "A: "}]
        )
        logger.info(str(whole_prompt))
        return whole_prompt
    else:
        raise ValueError(f"Invalid model name: {model_name}")


def generate_exp_shot_prompt(
    shot_method: ShotSamplingType,
    sequence_length: int,
    model_name=DAVINCI_MODEL_NAME,
    base=10,
    shot=1,
    num_range=(0, 7),
    offset_range=(0, 7),
):
    """
    Generate a single shot prompt for a explanation.
    """
    if shot_method == ShotSamplingType.RANDOM:
        fn, sequence = generate_random_fn_sequence(
            sequence_functions, num_range, offset_range, sequence_length
        )
    else:
        raise ValueError(f"Invalid shot method: {shot_method}")

    if model_name in OpenAITextModels.list():
        text = "Q: "
        text += ",".join([str(x) for x in sequence])
        text += "\n"
        text += "Explanation: "
        text += fn
        text += "\n"
        text += "\n"
        return text

    elif model_name in OpenAIChatModels.list():
        if base == 10 or base == 2:
            q_text = ",".join([str(x) for x in sequence])
        else:
            raise ValueError(f"Invalid base: {base}")
        response = [{"role": "user", "content": q_text}]
        a_text = "Explanation: " + fn
        response += [{"role": "assistant", "content": a_text}]
        return response

    else:
        logger.info(f"model name is: {model_name}")
        raise ValueError(f"Invalid model name: {model_name}")


def parse_explanation(model_response: str) -> str:
    """
    Parse an explanation into a function.
    """
    # Splitting the string into lines
    lines = model_response.split("\n")

    # Initializing the variables with None
    x = ""

    # Looping over the lines
    for line in lines:
        # Splitting the line into key and value
        parts = line.split(": ", 1)
        if len(parts) == 2:
            key, value = parts
            # Saving the value based on the key
            if key == "Explanation":
                x = value
    return x
