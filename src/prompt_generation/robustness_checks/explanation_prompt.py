"""
Create a prompt to continue a sequence of numbers, in an arbitrary base.

Prompts will take the form:
--------------------------
    Here are some examples of sequence explanations, i.e. python functions
    which could have generated the preceding sequences. Assume the first
    number was generated by f(0), the second by f(1), and so on.
    Sequence: 2, 4, 6
    Explanation: lambda x: 2*(x+1)

    Sequence: 1, 2, 3, 4, 5
    Explanation: lambda x: (x+1)

    Sequence: 9, 16, 25, 36
    Explanation: lambda x: (x+3)**2

    ***EXPLANATION_PROMPT***

    Explanation:

"""


import random
from logging import getLogger
from typing import List, Optional, Union

from src.evals.utils import _generate_random_function, reformat_function
from src.models.openai_model import (
    DAVINCI_MODEL_NAME,
    OpenAIChatModels,
    OpenAITextModels,
)
from src.pipelines import ShotSamplingType
from src.pipelines.sequence_completions import sequence_functions
from src.prompt_generation import PromptBase, get_formatted_prompt
from src.prompt_generation.robustness_checks.distribution_prompt import TASK_PROMPTS
from src.prompt_generation.robustness_checks.utils import (
    extend_prompt,
    initialise_prompt,
    start_question,
)

logger = getLogger(__name__)

PRE_PROMPT = """
Here are some examples of sequence explanations, i.e. python functions
which generated the preceding sequences base {}. Assume the first number was generated by f(0),
the second by f(1), the third by f(2), and so on.
"""

# TODO: fix generating functions to include recursive progressions, an ok fix for now.
sequence_functions = sequence_functions.copy()


def create_explanation_prompt(
    sequence: List[int],
    task_prompt: str,
    model_name: str = DAVINCI_MODEL_NAME,
    base: int = 10,
    shots: int = 0,
    shot_method: ShotSamplingType = ShotSamplingType.RANDOM,
    role_prompt: Optional[str] = None,
    seed: int = 0,
    show_function_space: bool = False,
) -> Union[str, List[dict]]:
    """
    Create a prompt to continue a sequence of numbers.
    """
    random.seed(seed)
    sequence_length = len(sequence)
    prompt_text = initialise_prompt(model_name)
    # Generate the few shot examples
    if shots > 0:
        for _ in range(shots):
            # Note: we are using the sequence length implicitly specified by
            # the target sequence to generate the prompts.
            shot_prompt = generate_exp_shot_prompt(
                shot_method, sequence_length, model_name, base, seed
            )
            prompt_text = extend_prompt(prompt_text, shot_prompt)
            seed += 1

    # Generate the explanation prompt
    text = TASK_PROMPTS[task_prompt]["explanation"]
    text = start_question(text, sequence, base, role_prompt)
    pre_prompt = PRE_PROMPT
    pre_prompt = pre_prompt.format(base)
    # Combine together to form the final prompt
    if model_name in OpenAITextModels.list():
        assert isinstance(prompt_text, str)
        # Prepend to the shots
        pretext = pre_prompt + "\n"
        pretext += "\n"
        text = pretext + prompt_text + text
        text += "\n"
        text += "Explanation: "
        logger.info(f"Full Explanation Prompt:{text}")
        return text
    elif model_name in OpenAIChatModels.list():
        assert isinstance(prompt_text, list)
        if show_function_space:
            if base == 10:
                file_text = get_formatted_prompt(PromptBase.ROBUST_COMPLETION_BASE10)
            elif base == 2:
                file_text = get_formatted_prompt(PromptBase.ROBUST_COMPLETION_BASE2)
            else:
                raise ValueError(f"Invalid base: {base}")

            pretext = [
                {
                    "role": "system",
                    "content": file_text,
                }
            ]
        else:
            pretext = [
                {
                    "role": "system",
                    "content": pre_prompt,
                }
            ]

        whole_prompt = (
            pretext
            + prompt_text
            + [{"role": "user", "content": text}]
            + [{"role": "assistant", "content": "A: "}]
        )
        logger.info(str(whole_prompt))
        return whole_prompt
    else:
        raise ValueError(f"Invalid model name: {model_name}")


def generate_exp_shot_prompt(
    shot_method: ShotSamplingType,
    sequence_length: int,
    model_name=DAVINCI_MODEL_NAME,
    base=10,
    seed=0,
    shot=1,
    num_range=(0, 7),
    offset_range=(0, 7),
):
    """
    Generate a single shot prompt for a explanation.
    """
    if shot_method == ShotSamplingType.RANDOM:
        for i in range(6):
            fn, offset = _generate_random_function(
                sequence_functions, num_range, offset_range, seed
            )
            # Reformat fn to replace every x after the first with x+offset
            fn = reformat_function(fn, offset, base)
            try:
                assert isinstance(fn, str)
                sequence = [eval(fn)(x) for x in range(sequence_length)]
                break
            except RecursionError:
                logger.info(f"Recursion Error in generate_exp_shot_prompt: {fn}")
                seed += 1
                random.seed(seed)
                if i == 5:
                    raise ValueError(
                        "Kept generating improper recursive functions, try again!"
                    )
    else:
        raise ValueError(f"Invalid shot method: {shot_method}")

    if model_name in OpenAITextModels.list():
        sequence_str = ",".join([str(x) for x in sequence])
        text = get_formatted_prompt(
            PromptBase.EXPLANATION_SHOT_TEXT, {"sequence": sequence_str, "fn": fn}
        )
        return text

    elif model_name in OpenAIChatModels.list():
        if base == 10 or base == 2:
            q_text = ",".join([str(x) for x in sequence])
        else:
            raise ValueError(f"Invalid base: {base}")
        response = [{"role": "user", "content": q_text}]
        a_text = "Explanation: " + fn
        response += [{"role": "assistant", "content": a_text}]
        return response

    else:
        logger.info(f"model name is: {model_name}")
        raise ValueError(f"Invalid model name: {model_name}")


def parse_explanation(model_response: str, model_name: str) -> str:
    """
    Parse an explanation into a function.
    """
    if model_name in OpenAITextModels.list():
        return model_response
    elif model_name in OpenAIChatModels.list():
        # Splitting the string into lines
        lines = model_response.split("\n")

        # Initializing the variables with None
        x = ""

        # Looping over the lines
        for line in lines:
            # Splitting the line into key and value
            parts = line.split(": ", 1)
            if len(parts) == 2:
                key, value = parts
                # Saving the value based on the key
                if key == "Explanation":
                    x = value
        return x
