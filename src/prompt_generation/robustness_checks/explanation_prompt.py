"""
Create a prompt to continue a sequence of numbers, in an arbitrary base.

Prompts will take the form:
--------------------------
    Here are some examples of sequence explanations, i.e. python functions
    which could have generated the preceding sequences. Assume the first
    number was generated by f(0), the second by f(1), and so on.
    Sequence: 2, 4, 6
    Explanation: lambda x: 2*(x+1)

    Sequence: 1, 2, 3, 4, 5
    Explanation: lambda x: (x+1)

    Sequence: 9, 16, 25, 36
    Explanation: lambda x: (x+3)**2

    ***EXPLANATION_PROMPT***

    Explanation:

"""


import random
from logging import getLogger
from typing import List, Optional, Union

from src.evals.utils import _generate_random_function, reformat_function
from src.models.openai_model import (
    DAVINCI_MODEL_NAME,
    OpenAIChatModels,
    OpenAITextModels,
)
from src.pipelines import ShotSamplingType
from src.pipelines.sequence_completions import sequence_functions
from src.prompt_generation.robustness_checks.distribution_prompt import (
    ROLE_PROMPTS,
    TASK_PROMPTS,
)
from src.prompt_generation.robustness_checks.utils import extend_prompt

logger = getLogger(__name__)

PRE_PROMPT = """
Here are some examples of sequence explanations, i.e. python functions
which generated the preceding sequences base {}. Assume the first number was generated by f(0),
the second by f(1), the third by f(2), and so on.
"""

# TODO: fix generating functions to include recursive progressions, an ok fix for now.
sequence_functions = sequence_functions.copy()


def create_explanation_prompt(
    sequence: List[int],
    task_prompt: str,
    model_name: str = DAVINCI_MODEL_NAME,
    base: int = 10,
    shots: int = 0,
    shot_method: ShotSamplingType = ShotSamplingType.RANDOM,
    role_prompt: Optional[str] = None,
    seed: int = 0,
) -> Union[str, List[dict]]:
    """
    Create a prompt to continue a sequence of numbers.
    """
    random.seed(seed)
    sequence_length = len(sequence)
    if model_name in OpenAITextModels.list():
        prompt_text = ""
    elif model_name in OpenAIChatModels.list():
        prompt_text = []
    else:
        raise ValueError(f"Invalid model name: {model_name}")
    if shots > 0:
        for _ in range(shots):
            # Note: we are using the sequence length implicitly specified by
            # the target sequence to generate the prompts.
            shot_prompt = generate_exp_shot_prompt(
                shot_method, sequence_length, model_name, base
            )
            prompt_text = extend_prompt(prompt_text, shot_prompt)

    text = TASK_PROMPTS[task_prompt]["explanation"]
    text += "\n"
    # TODO: Decide if we want role prompt to go here
    if role_prompt is not None:
        text += ROLE_PROMPTS[role_prompt]
        text += "\n"
    text += f"The sequence is in base {base}."
    text += "\nQ: "
    if base == 10:
        text += ",".join([str(x) for x in sequence])
    elif base == 2:
        text += ",".join([bin(x) for x in sequence])
    else:
        raise ValueError(f"Invalid base: {base}")
    pre_prompt = PRE_PROMPT
    pre_prompt = pre_prompt.format(base)
    # logger.info(pre_prompt)
    if model_name in OpenAITextModels.list():
        assert isinstance(prompt_text, str)
        # Prepend to the shots
        pretext = pre_prompt + "\n"
        pretext += "\n"
        text = pretext + prompt_text + text
        text += "\n"
        text += "A: "
        return text
    elif model_name in OpenAIChatModels.list():
        assert isinstance(prompt_text, list)
        pretext = [
            {
                "role": "system",
                "content": pre_prompt,
            }
        ]
        whole_prompt = (
            pretext
            + prompt_text
            + [{"role": "user", "content": text}]
            + [{"role": "assistant", "content": "A: "}]
        )
        logger.info(str(whole_prompt))
        return whole_prompt
    else:
        raise ValueError(f"Invalid model name: {model_name}")


def generate_exp_shot_prompt(
    shot_method: ShotSamplingType,
    sequence_length: int,
    model_name=DAVINCI_MODEL_NAME,
    base=10,
):
    """
    Generate a single shot prompt for a explanation.
    """
    if shot_method == "random":
        fn, offset = _generate_random_function(sequence_functions, (0, 7), (0, 7))
        # Reformat fn to replace every x after the first with x+offset
        fn = reformat_function(fn, offset, base)
        sequence = [eval(fn)(x) for x in range(sequence_length)]
    else:
        raise ValueError(f"Invalid shot method: {shot_method}")

    if model_name in OpenAITextModels.list():
        text = "Q: "
        text += ",".join([str(x) for x in sequence])
        text += "\n"
        text += "Explanation: "
        text += fn
        text += "\n"
        text += "\n"
        return text

    elif model_name in OpenAIChatModels.list():
        if base == 10 or base == 2:
            q_text = ",".join([str(x) for x in sequence])
        else:
            raise ValueError(f"Invalid base: {base}")
        response = [{"role": "user", "content": q_text}]
        a_text = "Explanation: " + fn
        response += [{"role": "assistant", "content": a_text}]
        return response

    else:
        logger.info(f"model name is: {model_name}")
        raise ValueError(f"Invalid model name: {model_name}")


def parse_explanation(model_response: str) -> str:
    """
    Parse an explanation into a function.
    """
    # Splitting the string into lines
    lines = model_response.split("\n")

    # Initializing the variables with None
    x = ""

    # Looping over the lines
    for line in lines:
        # Splitting the line into key and value
        parts = line.split(": ", 1)
        if len(parts) == 2:
            key, value = parts
            # Saving the value based on the key
            if key == "Explanation":
                x = value
    return x
