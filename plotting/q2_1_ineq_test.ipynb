{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dir = \"./results/q2_1/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(os.listdir(res_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_paths = []\n",
    "for par_dir, dirnames, _ in os.walk(res_dir):\n",
    "    for sub_dir in dirnames:\n",
    "        for dirpath, _, filenames in os.walk(os.path.join(par_dir, sub_dir)):\n",
    "            if \"results.csv\" in filenames:\n",
    "                csv_paths.append(os.path.join(dirpath, \"results.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_paths = list(set(csv_paths))\n",
    "csv_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df = pd.DataFrame()\n",
    "\n",
    "for csv_file in csv_paths:\n",
    "    df = pd.read_csv(csv_file, sep=\",\")\n",
    "    main_df = pd.concat([main_df, df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(csv_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(main_df))\n",
    "main_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(main_df[main_df[\"invalid_fn_type\"] == \"random\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df.groupby([\"num_shots\", \"invalid_fn_type\"])[\"test_passing_completion\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df.groupby([\"invalid_fn_type\"])[\"num_invalid\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df = main_df.pivot_table(\n",
    "    index=[\"num_shots\", \"invalid_fn_type\"], \n",
    "    values=[\n",
    "        \"test_passing_completion\", \"test_passing_explanation\", \n",
    "        \"org_func\",\n",
    "        ], \n",
    "    aggfunc={\n",
    "        \"test_passing_completion\": \"sum\",\n",
    "        \"test_passing_explanation\": \"sum\",\n",
    "        \"org_func\": \"count\",\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pivot_df.copy()\n",
    "final_df[\"test_passing_completion\"] = final_df[\"test_passing_completion\"] / final_df[\"org_func\"]\n",
    "final_df[\"test_passing_explanation\"] = final_df[\"test_passing_explanation\"] / final_df[\"org_func\"]\n",
    "final_df[\"n_examples\"] = final_df[\"org_func\"]\n",
    "final_df = final_df.drop(columns=[\"org_func\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(\"./results/q2_1/0711_q2_1_agg_ns4,6,8,10.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ungrouped_df = final_df.reset_index()\n",
    "ungrouped_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure and a set of subplots\n",
    "fig, axis = plt.subplots(1, ncols=2, figsize=(12, 6), sharey=\"row\")\n",
    "\n",
    "cols = [\"test_passing_completion\", \"test_passing_explanation\"]\n",
    "\n",
    "# The amount of space for each group of bars along the x-axis\n",
    "width = 0.2\n",
    "\n",
    "# The x locations for the groups\n",
    "x = np.arange(len(ungrouped_df['num_shots'].unique()))\n",
    "\n",
    "for idx, col in enumerate(cols):\n",
    "    ax = axis[idx]\n",
    "\n",
    "    # Divide the data into classes\n",
    "    compl_a = ungrouped_df[ungrouped_df['invalid_fn_type'] == 'exclude_class'][col]\n",
    "    compl_b = ungrouped_df[ungrouped_df['invalid_fn_type'] == 'same_class'][col]\n",
    "    compl_c = ungrouped_df[ungrouped_df['invalid_fn_type'] == 'random'][col]\n",
    "\n",
    "    # Create the lines\n",
    "    rects1 = ax.plot(x, compl_a, label='exclude_class', marker='o')\n",
    "    rects2 = ax.plot(x, compl_b, label='same_class', marker='o')\n",
    "    rects3 = ax.plot(x, compl_c, label='random_class', marker='o')\n",
    "\n",
    "    ax.set_xlabel('Number of Shots')\n",
    "    \n",
    "    if idx == 0:\n",
    "        ax.set_ylabel('Test Passing Rate')\n",
    "        ax.legend(loc='lower right')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(ungrouped_df['num_shots'].unique())\n",
    "    ax.title.set_text(col)\n",
    "\n",
    "st = fig.suptitle(\"Test Passing by Number of Shots and Invalid Function Type\", fontsize=\"x-large\")\n",
    "\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logprob Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_paths = []\n",
    "for par_dir, dirnames, _ in os.walk(res_dir):\n",
    "    for sub_dir in dirnames:\n",
    "        for dirpath, _, filenames in os.walk(os.path.join(par_dir, sub_dir)):\n",
    "            if \"logprobs.csv\" in filenames:\n",
    "                csv_paths.append(os.path.join(dirpath, \"logprobs.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_paths = list(set(csv_paths))\n",
    "csv_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df = pd.DataFrame()\n",
    "\n",
    "for csv_file in csv_paths:\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file, sep=\",\")\n",
    "    except:\n",
    "        print(\"Error reading file: {}\".format(csv_file))\n",
    "    main_df = pd.concat([main_df, df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(main_df))\n",
    "main_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df.to_csv(\"./results/q2_1/0718_q2_1_logprobs_agg_ns4,6,8,10.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df.groupby([\"num_shots\", \"invalid_fn_type\", \"response_type\", \"valid\"])[\"logprob\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pivot_df = main_df.pivot_table(\n",
    "    index=[\"num_shots\", \"invalid_fn_type\", \"response_type\", \"valid\"], \n",
    "    values=[\n",
    "        \"logprob\",\n",
    "        ], \n",
    "    aggfunc={\n",
    "        \"logprob\": [np.mean, \"std\", \"count\", \"min\", \"max\"],\n",
    "        \n",
    "    })\n",
    "\n",
    "pivot_df.columns = [f'{aggfunc}_{column}' for column, aggfunc in pivot_df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the dataframe according to your conditions\n",
    "response_type = \"completion\"\n",
    "df = main_df[(main_df['response_type'] == response_type)]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# define the valid types\n",
    "valid_types = df['valid'].unique()\n",
    "\n",
    "# For each type of validity, create a histogram\n",
    "for valid_type in valid_types:\n",
    "    valid_df = df[df['valid'] == valid_type]\n",
    "    \n",
    "    # If there is data for this combination\n",
    "    \n",
    "    sns.histplot(valid_df['logprob'], kde=True, label=valid_type, stat=\"count\", common_norm=False, \n",
    "            alpha=0.2, linewidth=.15)\n",
    "\n",
    "plt.xlabel('Log Probability')\n",
    "#plt.ylabel('Density')\n",
    "plt.title('Log Probability Distribution by Validity for \"{}\"'.format(response_type))\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the valid types\n",
    "cols = ['valid_and_pred', 'invalid_and_not_pred', 'valid_and_not_pred', 'invalid_and_pred']  #  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the dataframe according to your conditions\n",
    "\n",
    "#cols = ['valid_and_pred', 'invalid_and_not_pred', 'valid_and_not_pred']  #  \n",
    "\n",
    "num_shots = 10\n",
    "model = \"text-davinci-003\"\n",
    "response_type = \"completion\"\n",
    "\n",
    "df = main_df[(main_df['response_type'] == response_type) & (main_df[\"valid\"] != \"pred\") & (main_df['num_shots'] == num_shots) & (main_df[\"model\"] == model)]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# For each type of validity, create a histogram\n",
    "for col in cols:\n",
    "    valid_df = df[df[col] == 1]\n",
    "    \n",
    "    # If there is data for this combination\n",
    "    sns.histplot(valid_df['logprob'], kde=True, label=col, stat=\"density\", common_norm=False, \n",
    "            alpha=0.2, linewidth=.15)\n",
    "\n",
    "plt.xlabel('Log Probability')\n",
    "#plt.ylabel('Density')\n",
    "plt.title('Log Probability Distribution by Validity for \"{}\" (num_shots = {})'.format(response_type, num_shots))\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['valid_and_pred', 'valid_and_not_pred', 'invalid_and_not_pred', ]  #  \n",
    "\n",
    "# Create a figure and a set of subplots per num_shot value\n",
    "fig, axis = plt.subplots(2, 2, figsize=(20, 12), sharex=\"col\")\n",
    "ax_loc = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
    "\n",
    "response_type = \"completion\"\n",
    "shots = sorted(main_df[\"num_shots\"].unique())\n",
    "\n",
    "model = \"text-davinci-003\"\n",
    "df = main_df[(main_df['response_type'] == response_type) & (main_df[\"valid\"] != \"pred\") & (main_df[\"model\"] == model)]\n",
    "\n",
    "for n_shot, loc in zip(shots, ax_loc):\n",
    "    ax = axis[loc]\n",
    "\n",
    "    # For each type of validity, create a histogram\n",
    "    for col in cols:\n",
    "        \n",
    "        # select data for n_shot\n",
    "        valid_df = df[(df[\"num_shots\"] == n_shot) & (df[col] == 1)]\n",
    "        \n",
    "        # If there is data for this combination\n",
    "        sns.histplot(valid_df['logprob'], kde=True, label=col, stat=\"density\", common_norm=False, \n",
    "                alpha=0.2, linewidth=.15, ax=ax)\n",
    "\n",
    "    if loc[0] == 1:\n",
    "        ax.set_xlabel('Log Probability')\n",
    "    \n",
    "    ax.title.set_text(f\"num_shots = {n_shot}\")\n",
    "    ax.legend(loc='upper left')\n",
    "\n",
    "st = fig.suptitle(f\"Log Probability Distribution by Validity for '{response_type}' across num_shots\", fontsize=\"x-large\")\n",
    "st.set_y(0.95)\n",
    "fig.subplots_adjust(top=0.85)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cols = ['valid_and_pred', 'valid_and_not_pred', 'invalid_and_not_pred', ]  #  \n",
    "\n",
    "# filter the dataframe according to your conditions\n",
    "num_shots = 4\n",
    "model = \"text-davinci-003\"\n",
    "response_type = \"completion\"\n",
    "\n",
    "df = main_df[(main_df['response_type'] == response_type) & (main_df['num_shots'] == num_shots) & (main_df[\"model\"] == model)]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# define the valid types\n",
    "#cols = ['valid_and_pred', 'valid_and_not_pred', 'invalid_and_pred', 'invalid_and_not_pred']\n",
    "\n",
    "# For each type of validity, create a histogram\n",
    "for col in cols:\n",
    "    valid_df = df[df[col] == 1]\n",
    "    \n",
    "    # If there is data for this combination\n",
    "    sns.histplot(valid_df['logprob'], kde=True, label=col, stat=\"count\", common_norm=False, \n",
    "            alpha=0.2, linewidth=.15)\n",
    "\n",
    "plt.xlabel('Log Probability')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Log Probability Distribution by Validity for \"{}\" (num_shots = {})'.format(response_type, num_shots))\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['valid_and_pred', 'valid_and_not_pred', 'invalid_and_not_pred',]  #  \n",
    "\n",
    "# filter the dataframe according to your conditions\n",
    "#num_shots = 10\n",
    "model = \"text-davinci-003\"\n",
    "response_type = \"completion\"\n",
    "invalid_fn = \"random\"\n",
    "\n",
    "df = main_df[(main_df['response_type'] == response_type) & (main_df['invalid_fn_type'] == invalid_fn) & (main_df[\"valid\"] != \"pred\") & (main_df[\"model\"] == model)]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# For each type of validity, create a histogram\n",
    "for col in cols:\n",
    "    valid_df = df[df[col] == 1]\n",
    "    \n",
    "    # If there is data for this combination\n",
    "    sns.histplot(valid_df['logprob'], kde=True, label=col, stat=\"count\", common_norm=False, \n",
    "            alpha=0.2, linewidth=.15)\n",
    "\n",
    "plt.xlabel('Log Probability')\n",
    "#plt.ylabel('Density')\n",
    "plt.title('Log Probability Distribution by Validity for \"{}\" '.format(response_type))\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df[main_df['response_type'] == \"explanation\"][\"valid\"].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"total explantion: {}\".format(len(main_df[(main_df['response_type'] == \"explanation\")])))\n",
    "print(\"valid_and_not_pred: {}\".format(len(main_df[(main_df['response_type'] == \"explanation\") & (main_df[\"valid_and_not_pred\"] == 1)])))\n",
    "print(\"valid_and_pred: {}\".format(len(main_df[(main_df['response_type'] == \"explanation\") & (main_df[\"valid_and_pred\"] == 1)])))\n",
    "print(\"invalid_and_not_pred: {}\".format(len(main_df[(main_df['response_type'] == \"explanation\") & (main_df[\"invalid_and_not_pred\"] == 1)])))\n",
    "print(\"invalid_and_pred: {}\".format(len(main_df[(main_df['response_type'] == \"explanation\") & (main_df[\"invalid_and_pred\"] == 1)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_valid_and_pred_entries(\n",
    "    entry: dict, pred_val: str, valid_vals,\n",
    ") -> dict:\n",
    "    \"\"\"Determine variations of whether this entry was valid and predicted or not.\"\"\"\n",
    "\n",
    "    # update logprob entry\n",
    "    if entry[\"valid\"] in [\"valid\", \"invalid\"]:\n",
    "        entry[\"valid_and_pred\"] = (\n",
    "            1\n",
    "            if entry[\"valid\"] == \"valid\"\n",
    "            and entry[\"answer\"] == pred_val\n",
    "            else 0\n",
    "        )\n",
    "        entry[\"valid_and_not_pred\"] = (\n",
    "            1\n",
    "            if entry[\"valid\"] == \"valid\"\n",
    "            and entry[\"answer\"] != pred_val\n",
    "            else 0\n",
    "        )\n",
    "        entry[\"invalid_and_pred\"] = (\n",
    "            1\n",
    "            if entry[\"valid\"] == \"invalid\"\n",
    "            and entry[\"answer\"] == pred_val\n",
    "            else 0\n",
    "        )\n",
    "        entry[\"invalid_and_not_pred\"] = (\n",
    "            1\n",
    "            if entry[\"valid\"] == \"invalid\"\n",
    "            and entry[\"answer\"] != pred_val\n",
    "            else 0\n",
    "        )\n",
    "    else:\n",
    "        entry[\"valid_and_pred\"] = (\n",
    "            1 if entry[\"valid\"] == \"pred\" and pred_val in valid_vals else 0\n",
    "        )\n",
    "        entry[\"valid_and_not_pred\"] = 0\n",
    "        entry[\"invalid_and_pred\"] = (\n",
    "            1 if entry[\"valid\"] == \"pred\" and pred_val not in valid_vals else 0\n",
    "        )\n",
    "        entry[\"invalid_and_not_pred\"] = 0\n",
    "\n",
    "    return entry\n",
    "\n",
    "\n",
    "NEED_UPDATE = False\n",
    "\n",
    "if NEED_UPDATE:\n",
    "\n",
    "    # find valid answer options per sequence\n",
    "    df_expl = main_df[main_df['response_type'] == \"explanation\"]\n",
    "\n",
    "\n",
    "    grouped = df_expl.groupby([\"sequence\"])\n",
    "\n",
    "    list_of_dicts = []\n",
    "\n",
    "    for name, group in grouped:\n",
    "        valid_options = []\n",
    "        # convert group dataframe to dictionary\n",
    "        group_dict = group.to_dict(orient='records')\n",
    "        \n",
    "        \n",
    "        # get valid options\n",
    "        pred_val = -1\n",
    "        for elem in group_dict:\n",
    "            if elem[\"valid\"] == \"valid\":\n",
    "                valid_options.append(elem[\"answer\"])\n",
    "            elif elem[\"valid\"] == \"pred\":\n",
    "                pred_val = elem[\"answer\"]\n",
    "\n",
    "        # iterate over entry to update \n",
    "        for elem in group_dict:\n",
    "            elem = _get_valid_and_pred_entries(elem, pred_val, valid_options)\n",
    "            list_of_dicts.append(elem)\n",
    "\n",
    "        # append it to list with the group name as the key\n",
    "        # list_of_dicts.extend(group_dict)\n",
    "\n",
    "    df_expl = df_expl.from_dict(list_of_dicts, orient=\"columns\")\n",
    "    print(df_expl.info())\n",
    "\n",
    "    print(\"total: {}\".format(len(df_expl)))\n",
    "    print(\"valid_and_not_pred: {}\".format(len(df_expl[(df_expl['response_type'] == \"explanation\") & (df_expl[\"valid_and_not_pred\"] == 1)])))\n",
    "    print(\"valid_and_pred: {}\".format(len(df_expl[(df_expl['response_type'] == \"explanation\") & (df_expl[\"valid_and_pred\"] == 1)])))\n",
    "    print(\"invalid_and_not_pred: {}\".format(len(df_expl[(df_expl['response_type'] == \"explanation\") & (df_expl[\"invalid_and_not_pred\"] == 1)])))\n",
    "    print(\"invalid_and_pred: {}\".format(len(df_expl[(df_expl['response_type'] == \"explanation\") & (df_expl[\"invalid_and_pred\"] == 1)])))\n",
    "\n",
    "    df_compl = main_df[main_df[\"response_type\"] == \"completion\"]\n",
    "    main_df = pd.concat([df_expl, df_compl])\n",
    "    print(main_df.info())\n",
    "    # store update logprob df\n",
    "    main_df.to_csv(\"./results/q2_1/0718_q2_1_logprobs_agg_ns4,6,8,10.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the dataframe according to your conditions\n",
    "# response_type == \"explanation\"\n",
    "# df = main_df[main_df['response_type'] == response_type]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# define the valid types\n",
    "valid_types = df_expl['valid'].unique()\n",
    "\n",
    "# For each type of validity, create a histogram\n",
    "for valid_type in valid_types:\n",
    "    valid_df = df_expl[df_expl['valid'] == valid_type]\n",
    "    \n",
    "    # If there is data for this combination\n",
    "    \n",
    "    sns.histplot(valid_df['logprob'], kde=True, label=valid_type, stat=\"count\", common_norm=False, \n",
    "            alpha=0.2, linewidth=.15)\n",
    "\n",
    "plt.xlabel('Log Probability')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Log Probability Distribution by Validity for \"explanation\"')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the dataframe according to your conditions\n",
    "num_shots = 4\n",
    "model = \"text-davinci-003\"\n",
    "response_type = \"explanation\"\n",
    "\n",
    "df = df_expl[(df_expl['num_shots'] == num_shots) & (df_expl[\"valid\"] != \"pred\") & (main_df[\"model\"] == model)]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# define the valid types\n",
    "cols = ['valid_and_pred', 'valid_and_not_pred', 'invalid_and_pred', 'invalid_and_not_pred']\n",
    "\n",
    "# For each type of validity, create a histogram\n",
    "for col in cols:\n",
    "    valid_df = df[df[col] == 1]\n",
    "    \n",
    "    # If there is data for this combination\n",
    "    sns.histplot(valid_df['logprob'], kde=True, label=col, stat=\"density\", common_norm=False, \n",
    "            alpha=0.2, linewidth=.15)\n",
    "\n",
    "plt.xlabel('Log Probability')\n",
    "\n",
    "plt.title('Log Probability Distribution by Validity for \"{}\" (num_shots = {})'.format(response_type, num_shots))\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_and_pred_ratio(df, response_type):\n",
    "    df_res = df[df[\"response_type\"] == response_type]\n",
    "    total = len(df_res[df_res[\"valid\"] != \"pred\"])\n",
    "    print(\"response type: {}\".format(response_type))\n",
    "    print(\"total: {}\".format(total))\n",
    "    print(\"valid_and_pred: {:.2f}\".format(len(df_res[df_res[\"valid_and_pred\"] == 1])/ total))\n",
    "    print(\"valid_and_not_pred: {:.2f}\".format(len(df_res[df_res[\"valid_and_not_pred\"] == 1])/ total))\n",
    "    print(\"invalid_and_pred: {:.2f}\".format(len(df_res[df_res[\"invalid_and_pred\"] == 1])/ total))\n",
    "    print(\"invalid_and_not_pred: {:.2f}\".format(len(df_res[df_res[\"invalid_and_not_pred\"] == 1])/ total))\n",
    "\n",
    "    total_valid = len(df_res[df_res[\"valid\"] == \"valid\"])\n",
    "    total_invalid = len(df_res[df_res[\"valid\"] == \"invalid\"])\n",
    "    print(\"total valid: {}\".format(total_valid))\n",
    "    print(\"total invalid: {}\".format(total_invalid))\n",
    "    print(\"valid_and_pred out of valid: {:.2f}\".format(len(df_res[df_res[\"valid_and_pred\"] == 1]) / total_valid))\n",
    "    print(\"invalid_and_pred out of invalid: {:.2f}\".format(len(df_res[df_res[\"invalid_and_pred\"] == 1]) / total_invalid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure and a set of subplots per num_shot value\n",
    "fig, axis = plt.subplots(2, 2, figsize=(20, 12), sharex=\"col\")\n",
    "ax_loc = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
    "\n",
    "response_type = \"explanation\"\n",
    "shots = sorted(df_expl[\"num_shots\"].unique())\n",
    "\n",
    "model = \"text-davinci-003\"\n",
    "df = df_expl[(df_expl[\"model\"] == model) & (df_expl[\"valid\"] != \"pred\")]\n",
    "\n",
    "# define the valid types\n",
    "cols = ['valid_and_pred', 'valid_and_not_pred', 'invalid_and_not_pred', 'invalid_and_pred']  #  \n",
    "\n",
    "for n_shot, loc in zip(shots, ax_loc):\n",
    "    ax = axis[loc]\n",
    "\n",
    "    # For each type of validity, create a histogram\n",
    "    for col in cols:\n",
    "        \n",
    "        # select data for n_shot\n",
    "        valid_df = df[(df[\"num_shots\"] == n_shot) & (df[col] == 1)]\n",
    "        \n",
    "        # If there is data for this combination\n",
    "        sns.histplot(valid_df['logprob'], kde=True, label=col, stat=\"density\", common_norm=False, \n",
    "                alpha=0.2, linewidth=.15, ax=ax)\n",
    "\n",
    "    #ax.set_ylabel('Density')\n",
    "    if loc[0] == 1:\n",
    "        ax.set_xlabel('Log Probability')\n",
    "    #ax.set_xticks(x)\n",
    "    #ax.set_xticklabels(ungrouped_df['num_shots'].unique())\n",
    "    ax.title.set_text(f\"num_shots = {n_shot}\")\n",
    "    ax.legend(loc='upper left')\n",
    "\n",
    "# Add some text for labels, title, and custom x-axis tick labels\n",
    "\n",
    "\n",
    "st = fig.suptitle(f\"Log Probability Distribution by Validity for '{response_type}' across num_shots\", fontsize=\"x-large\")\n",
    "st.set_y(0.95)\n",
    "fig.subplots_adjust(top=0.85)\n",
    "\n",
    "# fig.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the dataframe according to your conditions\n",
    "\n",
    "model = \"text-davinci-003\"\n",
    "response_type = \"explanation\"\n",
    "\n",
    "df = df_expl[(df_expl[\"valid\"] != \"pred\") & (main_df[\"model\"] == model)]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# define the valid types\n",
    "cols = ['valid_and_pred', 'valid_and_not_pred', 'invalid_and_pred', 'invalid_and_not_pred']\n",
    "\n",
    "# For each type of validity, create a histogram\n",
    "for col in cols:\n",
    "    valid_df = df[df[col] == 1]\n",
    "    \n",
    "    # If there is data for this combination\n",
    "    sns.histplot(valid_df['logprob'], kde=True, label=col, stat=\"density\", common_norm=False, \n",
    "            alpha=0.2, linewidth=.15)\n",
    "\n",
    "plt.xlabel('Log Probability')\n",
    "\n",
    "plt.title('Log Probability Distribution by Validity for \"{}\"'.format(response_type, num_shots))\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Out of the predicted values, how many are valid and invalid?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index: num_shots, sequence, invalid_fn_type\n",
    "# count: valid_and_pred, etc.\n",
    "# normalise by: total_valid_invalid\n",
    "\n",
    "pivot_df = main_df[(main_df[\"valid\"] == \"pred\")].pivot_table(\n",
    "    index=[\"num_shots\", \"invalid_fn_type\", \"response_type\"], \n",
    "    values=[\n",
    "        'valid_and_pred', 'invalid_and_not_pred', 'valid_and_not_pred', 'invalid_and_pred'\n",
    "        ], \n",
    "    aggfunc=\"sum\")\n",
    "\n",
    "pivot_df[\"total\"] = pivot_df['valid_and_pred'] + pivot_df['valid_and_not_pred'] + pivot_df['invalid_and_pred'] + pivot_df['invalid_and_not_pred']\n",
    "pivot_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['valid_and_pred',  'invalid_and_pred']\n",
    "for col in cols:\n",
    "    pivot_df[col] = round(pivot_df[col] / pivot_df[\"total\"], 3)\n",
    "\n",
    "pivot_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pivot_df.drop(['invalid_and_not_pred', 'valid_and_not_pred'], axis=1)\n",
    "\n",
    "df.to_latex(\"results/q2_1/0719_pred_valid_and_invalid_ratio.tex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310_aisc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
