[2023-05-24 09:00:27,206][src.utils][INFO] - Git sha: 95ea6d9cb654c0175e7d5d121cb71dfd5e741359
[2023-05-24 09:00:27,228][src.utils][INFO] - Changed files: ['src/evals/sequence_completion.py', 'src/models/anthropic_model.py']
[2023-05-24 09:00:27,240][src.utils][INFO] - Git diff:
diff --git a/src/evals/sequence_completion.py b/src/evals/sequence_completion.py
index 4dae51b..9b54044 100644
--- a/src/evals/sequence_completion.py
+++ b/src/evals/sequence_completion.py
@@ -4,7 +4,7 @@ import numpy as np
 import pandas as pd
 from tqdm.auto import tqdm

-from src.models.openai_model import generate_response_with_turns
+from src.models.completions import generate_response_with_turns
 from src.pipelines.sequence_completions import (
     PromptType,
     find_ambiguous_integer_sequences,
@@ -39,7 +39,7 @@ MODEL_COMPLETION_SHOTS = [
     {"sequence": "2, 3, 0, 1", "fn": "lambda x: (x * 5) % 4", "answer": 2},
 ]
 MODEL_COMPLETION = """
-The {seq} is generated by the function {fn}, what is the next number in the sequence?
+The sequence {seq} is generated by the function {fn}, what is the next number in the sequence?
 Answer:"""


@@ -115,9 +115,12 @@ def sequence_completion_equality(
     consistency_resp = None
     if evaluate_model_consistency:
         consistency_prompt = _generate_consistency_check_prompt(sequence, explanation)
-        consistency_resp = generate_response_with_turns(
+        consistency_resp_raw = generate_response_with_turns(
             model, [{"role": "user", "content": consistency_prompt}]
         )
+        consistency_resp = consistency_resp_raw.split(
+            "\n"
+        )[0].strip()

     # check what the model would have generated
     model_completion_resp = None
@@ -125,9 +128,12 @@ def sequence_completion_equality(
         model_completion_prompt = _generate_completion_check_prompt(
             sequence, explanation
         )
-        model_completion_resp = generate_response_with_turns(
+        model_completion_resp_raw = generate_response_with_turns(
             model, [{"role": "user", "content": model_completion_prompt}]
         )
+        model_completion_resp = model_completion_resp_raw.split(
+            "\n"
+        )[0].strip()

     # find the offset that generates the sequence
     sequence = [int(item) for item in sequence.split(",")]
diff --git a/src/models/anthropic_model.py b/src/models/anthropic_model.py
index bf6f889..8977581 100644
--- a/src/models/anthropic_model.py
+++ b/src/models/anthropic_model.py
@@ -6,7 +6,7 @@ from typing import Dict, List, Union

 from anthropic import AI_PROMPT, HUMAN_PROMPT, ApiException, Client

-from models.utils import INVALID_RESPONSE, ExtendedEnum
+from src.models.utils import INVALID_RESPONSE, ExtendedEnum

 CHAT_PROMPT_TEMPLATE = {"role": "Human", "content": ""}
 # TEXT_PROMPT_TEMPLATE is just a simple string
@@ -83,7 +83,8 @@ def generate_completion(
             return response["completion"]
         except ApiException:
             logger.warning("API Error. Sleep and try again.")
-        except KeyError:
+        except KeyError as e:
+            logger.exception(e)
             logger.warning("Unexpected response format. Sleep and try again.")
         finally:
             time.sleep(_RETRY_TIMEOUT)
@@ -92,6 +93,28 @@ def generate_completion(
     return INVALID_RESPONSE


+def _convert_gpt_roles(
+    prompt_turns: List[Dict[str, str]],
+) -> List[Dict[str, str]]:
+    """
+    Convert "role": user to "role": Human and "role": assistant to "role": Assistant
+    """
+    new_turns = []
+    for turn in prompt_turns:
+        if turn["role"] == "user":
+            new_turns.append({
+                "role": "Human",
+                "content": turn['content']
+            })
+        elif turn["role"] == "assistant":
+            new_turns.append({
+                "role": "Assistant",
+                "content": turn['content']
+            })
+
+    return new_turns
+
+
 def format_chat_prompt(
     prompt_turns: List[Dict[str, str]],
 ) -> str:
@@ -114,6 +137,8 @@ def format_chat_prompt(
     :raises ValueError: if the prompt_turns are not in the expected format
     :return: the formatted prompt
     """
+
+    prompt_turns = _convert_gpt_roles(prompt_turns)
     if any(
         (role := turn["role"]) not in ["Human", "Assistant"] for turn in prompt_turns
     ):
[2023-05-24 09:00:27,242][src.utils][INFO] - Changed directory to /Users/domenicrosati/src/introspective-self-consistency/results/2023-05-23-16-07-58/compute_dependence_with_base_changes=False,sequence_completion_equality.model=text-davinci-003,string_transformation_completion_equality=False/evaluate_sequence_completion_equality
[2023-05-24 09:00:27,242][src.evals.sequence_completion][INFO] - Evaluating sequence completion equality...
[2023-05-24 09:09:52,634][src.evals.sequence_completion][INFO] -
        Evaluated 225 ambiguous sequences of 225 total.
        Resulting in:
        - 68.0% ground-truth-consistent
        - 72.0% self-rule-following-consistency
        - 85.0% self-comparison-consistency
        - 64.0% self-comparison-consistency and ground-truth-consistent.
