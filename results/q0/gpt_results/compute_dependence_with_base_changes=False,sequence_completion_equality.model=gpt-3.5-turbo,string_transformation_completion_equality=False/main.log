[2023-05-24 09:09:52,857][src.utils][INFO] - Git sha: 95ea6d9cb654c0175e7d5d121cb71dfd5e741359
[2023-05-24 09:09:52,874][src.utils][INFO] - Changed files: ['src/evals/sequence_completion.py', 'src/models/anthropic_model.py']
[2023-05-24 09:09:52,887][src.utils][INFO] - Git diff:
diff --git a/src/evals/sequence_completion.py b/src/evals/sequence_completion.py
index 4dae51b..9b54044 100644
--- a/src/evals/sequence_completion.py
+++ b/src/evals/sequence_completion.py
@@ -4,7 +4,7 @@ import numpy as np
 import pandas as pd
 from tqdm.auto import tqdm

-from src.models.openai_model import generate_response_with_turns
+from src.models.completions import generate_response_with_turns
 from src.pipelines.sequence_completions import (
     PromptType,
     find_ambiguous_integer_sequences,
@@ -39,7 +39,7 @@ MODEL_COMPLETION_SHOTS = [
     {"sequence": "2, 3, 0, 1", "fn": "lambda x: (x * 5) % 4", "answer": 2},
 ]
 MODEL_COMPLETION = """
-The {seq} is generated by the function {fn}, what is the next number in the sequence?
+The sequence {seq} is generated by the function {fn}, what is the next number in the sequence?
 Answer:"""


@@ -115,9 +115,12 @@ def sequence_completion_equality(
     consistency_resp = None
     if evaluate_model_consistency:
         consistency_prompt = _generate_consistency_check_prompt(sequence, explanation)
-        consistency_resp = generate_response_with_turns(
+        consistency_resp_raw = generate_response_with_turns(
             model, [{"role": "user", "content": consistency_prompt}]
         )
+        consistency_resp = consistency_resp_raw.split(
+            "\n"
+        )[0].strip()

     # check what the model would have generated
     model_completion_resp = None
@@ -125,9 +128,12 @@ def sequence_completion_equality(
         model_completion_prompt = _generate_completion_check_prompt(
             sequence, explanation
         )
-        model_completion_resp = generate_response_with_turns(
+        model_completion_resp_raw = generate_response_with_turns(
             model, [{"role": "user", "content": model_completion_prompt}]
         )
+        model_completion_resp = model_completion_resp_raw.split(
+            "\n"
+        )[0].strip()

     # find the offset that generates the sequence
     sequence = [int(item) for item in sequence.split(",")]
diff --git a/src/models/anthropic_model.py b/src/models/anthropic_model.py
index bf6f889..8977581 100644
--- a/src/models/anthropic_model.py
+++ b/src/models/anthropic_model.py
@@ -6,7 +6,7 @@ from typing import Dict, List, Union

 from anthropic import AI_PROMPT, HUMAN_PROMPT, ApiException, Client

-from models.utils import INVALID_RESPONSE, ExtendedEnum
+from src.models.utils import INVALID_RESPONSE, ExtendedEnum

 CHAT_PROMPT_TEMPLATE = {"role": "Human", "content": ""}
 # TEXT_PROMPT_TEMPLATE is just a simple string
@@ -83,7 +83,8 @@ def generate_completion(
             return response["completion"]
         except ApiException:
             logger.warning("API Error. Sleep and try again.")
-        except KeyError:
+        except KeyError as e:
+            logger.exception(e)
             logger.warning("Unexpected response format. Sleep and try again.")
         finally:
             time.sleep(_RETRY_TIMEOUT)
@@ -92,6 +93,28 @@ def generate_completion(
     return INVALID_RESPONSE


+def _convert_gpt_roles(
+    prompt_turns: List[Dict[str, str]],
+) -> List[Dict[str, str]]:
+    """
+    Convert "role": user to "role": Human and "role": assistant to "role": Assistant
+    """
+    new_turns = []
+    for turn in prompt_turns:
+        if turn["role"] == "user":
+            new_turns.append({
+                "role": "Human",
+                "content": turn['content']
+            })
+        elif turn["role"] == "assistant":
+            new_turns.append({
+                "role": "Assistant",
+                "content": turn['content']
+            })
+
+    return new_turns
+
+
 def format_chat_prompt(
     prompt_turns: List[Dict[str, str]],
 ) -> str:
@@ -114,6 +137,8 @@ def format_chat_prompt(
     :raises ValueError: if the prompt_turns are not in the expected format
     :return: the formatted prompt
     """
+
+    prompt_turns = _convert_gpt_roles(prompt_turns)
     if any(
         (role := turn["role"]) not in ["Human", "Assistant"] for turn in prompt_turns
     ):
[2023-05-24 09:09:52,889][src.utils][INFO] - Changed directory to /Users/domenicrosati/src/introspective-self-consistency/results/2023-05-23-16-07-58/compute_dependence_with_base_changes=False,sequence_completion_equality.model=gpt-3.5-turbo,string_transformation_completion_equality=False/evaluate_sequence_completion_equality
[2023-05-24 09:09:52,889][src.evals.sequence_completion][INFO] - Evaluating sequence completion equality...
[2023-05-24 09:10:23,619][src.evals.sequence_completion][ERROR] - invalid literal for int() with base 10: '0 (since the sequence will repeat itself)'
Traceback (most recent call last):
  File "/Users/domenicrosati/src/introspective-self-consistency/src/evals/sequence_completion.py", line 191, in evaluate_sequence_completion_equality
    sequence_completion_equality(
  File "/Users/domenicrosati/src/introspective-self-consistency/src/evals/sequence_completion.py", line 171, in sequence_completion_equality
    "model_completion_matches": int(model_completion_resp) == last_completion,
ValueError: invalid literal for int() with base 10: '0 (since the sequence will repeat itself)'
[2023-05-24 09:10:23,620][src.evals.sequence_completion][WARNING] - invalid literal for int() with base 10: '0 (since the sequence will repeat itself)'
[2023-05-24 09:10:29,808][src.evals.sequence_completion][ERROR] - invalid literal for int() with base 10: '0 (since the sequence repeats every 4 numbers)'
Traceback (most recent call last):
  File "/Users/domenicrosati/src/introspective-self-consistency/src/evals/sequence_completion.py", line 191, in evaluate_sequence_completion_equality
    sequence_completion_equality(
  File "/Users/domenicrosati/src/introspective-self-consistency/src/evals/sequence_completion.py", line 171, in sequence_completion_equality
    "model_completion_matches": int(model_completion_resp) == last_completion,
ValueError: invalid literal for int() with base 10: '0 (since the sequence repeats every 4 numbers)'
[2023-05-24 09:10:29,809][src.evals.sequence_completion][WARNING] - invalid literal for int() with base 10: '0 (since the sequence repeats every 4 numbers)'
[2023-05-24 09:12:02,647][src.evals.sequence_completion][ERROR] - invalid literal for int() with base 10: '0 (since the sequence repeats every 4 numbers)'
Traceback (most recent call last):
  File "/Users/domenicrosati/src/introspective-self-consistency/src/evals/sequence_completion.py", line 191, in evaluate_sequence_completion_equality
    sequence_completion_equality(
  File "/Users/domenicrosati/src/introspective-self-consistency/src/evals/sequence_completion.py", line 171, in sequence_completion_equality
    "model_completion_matches": int(model_completion_resp) == last_completion,
ValueError: invalid literal for int() with base 10: '0 (since the sequence repeats every 4 numbers)'
[2023-05-24 09:12:02,647][src.evals.sequence_completion][WARNING] - invalid literal for int() with base 10: '0 (since the sequence repeats every 4 numbers)'
[2023-05-24 09:12:06,675][src.evals.sequence_completion][ERROR] - invalid literal for int() with base 10: '0 (since the sequence repeats itself)'
Traceback (most recent call last):
  File "/Users/domenicrosati/src/introspective-self-consistency/src/evals/sequence_completion.py", line 191, in evaluate_sequence_completion_equality
    sequence_completion_equality(
  File "/Users/domenicrosati/src/introspective-self-consistency/src/evals/sequence_completion.py", line 171, in sequence_completion_equality
    "model_completion_matches": int(model_completion_resp) == last_completion,
ValueError: invalid literal for int() with base 10: '0 (since the sequence repeats itself)'
[2023-05-24 09:12:06,675][src.evals.sequence_completion][WARNING] - invalid literal for int() with base 10: '0 (since the sequence repeats itself)'
[2023-05-24 09:12:10,286][src.evals.sequence_completion][ERROR] - invalid literal for int() with base 10: '1 (the sequence repeats every 4 numbers)'
Traceback (most recent call last):
  File "/Users/domenicrosati/src/introspective-self-consistency/src/evals/sequence_completion.py", line 191, in evaluate_sequence_completion_equality
    sequence_completion_equality(
  File "/Users/domenicrosati/src/introspective-self-consistency/src/evals/sequence_completion.py", line 171, in sequence_completion_equality
    "model_completion_matches": int(model_completion_resp) == last_completion,
ValueError: invalid literal for int() with base 10: '1 (the sequence repeats every 4 numbers)'
[2023-05-24 09:12:10,286][src.evals.sequence_completion][WARNING] - invalid literal for int() with base 10: '1 (the sequence repeats every 4 numbers)'
[2023-05-24 09:12:11,902][src.evals.sequence_completion][ERROR] - list index out of range
Traceback (most recent call last):
  File "/Users/domenicrosati/src/introspective-self-consistency/src/evals/sequence_completion.py", line 191, in evaluate_sequence_completion_equality
    sequence_completion_equality(
  File "/Users/domenicrosati/src/introspective-self-consistency/src/evals/sequence_completion.py", line 162, in sequence_completion_equality
    last_completion = eval(explanation)(last_completion_step + 1)
  File "<string>", line 1, in <lambda>
IndexError: list index out of range
[2023-05-24 09:12:11,902][src.evals.sequence_completion][WARNING] - list index out of range
[2023-05-24 09:13:08,463][src.evals.sequence_completion][ERROR] - invalid syntax (<string>, line 1)
Traceback (most recent call last):
  File "/Users/domenicrosati/src/introspective-self-consistency/src/evals/sequence_completion.py", line 191, in evaluate_sequence_completion_equality
    sequence_completion_equality(
  File "/Users/domenicrosati/src/introspective-self-consistency/src/evals/sequence_completion.py", line 143, in sequence_completion_equality
    completion = eval(explanation)(i)
  File "<string>", line 1
    This sequence is not an arithmetic or geometric sequence. It is the sequence of prime numbers starting from 2 with the first term removed. There is no mathematical formula to generate prime numbers, but there are algorithms to generate them.
         ^^^^^^^^
SyntaxError: invalid syntax
[2023-05-24 09:13:08,463][src.evals.sequence_completion][WARNING] - invalid syntax (<string>, line 1)
[2023-05-24 09:13:15,085][src.evals.sequence_completion][ERROR] - invalid syntax (<string>, line 1)
Traceback (most recent call last):
  File "/Users/domenicrosati/src/introspective-self-consistency/src/evals/sequence_completion.py", line 191, in evaluate_sequence_completion_equality
    sequence_completion_equality(
  File "/Users/domenicrosati/src/introspective-self-consistency/src/evals/sequence_completion.py", line 143, in sequence_completion_equality
    completion = eval(explanation)(i)
  File "<string>", line 1
    This sequence is not an arithmetic or geometric sequence. It is the Fibonacci sequence. The code to generate the sequence is:
         ^^^^^^^^
SyntaxError: invalid syntax
[2023-05-24 09:13:15,085][src.evals.sequence_completion][WARNING] - invalid syntax (<string>, line 1)
[2023-05-24 09:13:31,992][src.evals.sequence_completion][ERROR] - list index out of range
Traceback (most recent call last):
  File "/Users/domenicrosati/src/introspective-self-consistency/src/evals/sequence_completion.py", line 191, in evaluate_sequence_completion_equality
    sequence_completion_equality(
  File "/Users/domenicrosati/src/introspective-self-consistency/src/evals/sequence_completion.py", line 162, in sequence_completion_equality
    last_completion = eval(explanation)(last_completion_step + 1)
  File "<string>", line 1, in <lambda>
IndexError: list index out of range
[2023-05-24 09:13:31,993][src.evals.sequence_completion][WARNING] - list index out of range
[2023-05-24 09:14:27,078][src.evals.sequence_completion][ERROR] - list index out of range
Traceback (most recent call last):
  File "/Users/domenicrosati/src/introspective-self-consistency/src/evals/sequence_completion.py", line 191, in evaluate_sequence_completion_equality
    sequence_completion_equality(
  File "/Users/domenicrosati/src/introspective-self-consistency/src/evals/sequence_completion.py", line 162, in sequence_completion_equality
    last_completion = eval(explanation)(last_completion_step + 1)
  File "<string>", line 1, in <lambda>
IndexError: list index out of range
[2023-05-24 09:14:27,079][src.evals.sequence_completion][WARNING] - list index out of range
[2023-05-24 09:16:02,135][src.evals.sequence_completion][INFO] -
        Evaluated 215 ambiguous sequences of 225 total.
        Resulting in:
        - 83.0% ground-truth-consistent
        - 84.0% self-rule-following-consistency
        - 94.0% self-comparison-consistency
        - 79.0% self-comparison-consistency and ground-truth-consistent.
