[2023-05-26 11:18:36,306][src.utils][INFO] - Git sha: 801edc7a631335386ba3c9413a1e5c6be97303f7
[2023-05-26 11:18:36,318][src.utils][INFO] - Changed files: ['conf/tasks/compute_dependence_with_base_changes.yaml', 'src/evals/evaluate_continuation.py', 'src/evals/evaluate_explanation.py', 'src/evals/prompts/continuation_prompt.py', 'src/evals/prompts/explanation_prompt.py', 'src/evals/sequence_completion_with_base_change.py', 'src/models/openai_model.py']
[2023-05-26 11:18:36,334][src.utils][INFO] - Git diff:
diff --git a/conf/tasks/compute_dependence_with_base_changes.yaml b/conf/tasks/compute_dependence_with_base_changes.yaml
index 29b202c..6be90fc 100644
--- a/conf/tasks/compute_dependence_with_base_changes.yaml
+++ b/conf/tasks/compute_dependence_with_base_changes.yaml
@@ -1,7 +1,7 @@
 sequence_type: binary
-model: gpt-3.5-turbo
+model: gpt-4-0314
 num_shots: 4
 on_ambiguous_sequences: true
 num_samples: 1
-distribution: forced-consistency
+distribution: default
 shot_method: random
diff --git a/src/evals/evaluate_continuation.py b/src/evals/evaluate_continuation.py
index 65788a6..e1f7f45 100644
--- a/src/evals/evaluate_continuation.py
+++ b/src/evals/evaluate_continuation.py
@@ -7,6 +7,8 @@ from src.models.openai_model import (
     generate_completion,
 )
 
+from src.models.openai_model import OpenAITextModels, OpenAIChatModels
+
 
 def valid_continuation(
     model_continuation: str,
@@ -23,6 +25,7 @@ def valid_continuation(
         if base == 10:
             int(model_continuation)
         elif base == 2:
+
             int(model_continuation[2:], 2)
     except ValueError:
         return False
@@ -38,7 +41,7 @@ def generate_continuation(
     """
     Given a prompt, generate a continuation from the model.
     """
-    if model_name == "text-davinci-003":
+    if model_name in OpenAITextModels.list():
         # Feed this into the model
         model_response = generate_completion(
             prompt=prompt,
@@ -46,7 +49,7 @@ def generate_continuation(
             max_tokens=256,
             model=DAVINCI_MODEL_NAME,
         )
-    elif model_name == "gpt-3.5-turbo":
+    elif model_name in OpenAIChatModels.list():
         # Feed this into the model
         model_response = generate_chat_completion(
             prompt_turns=prompt,
diff --git a/src/evals/evaluate_explanation.py b/src/evals/evaluate_explanation.py
index e1f12d4..ff3071f 100644
--- a/src/evals/evaluate_explanation.py
+++ b/src/evals/evaluate_explanation.py
@@ -7,6 +7,8 @@ from src.models.openai_model import (
     generate_completion,
 )
 
+from src.models.openai_model import OpenAITextModels, OpenAIChatModels
+
 
 def valid_explanation(
     fn_form: str,
@@ -55,7 +57,7 @@ def generate_explanation(
     Given a prompt, generate an explanation from the model.
     TODO: refactor code, entirely copied from generate_continuation
     """
-    if model_name == "text-davinci-003":
+    if model_name in OpenAITextModels.list():
         # Feed this into the model
         model_response = generate_completion(
             prompt=prompt,
@@ -63,7 +65,7 @@ def generate_explanation(
             max_tokens=256,
             model=DAVINCI_MODEL_NAME,
         )
-    elif model_name == "gpt-3.5-turbo":
+    elif model_name in OpenAIChatModels.list():
         # Feed this into the model
         model_response = generate_chat_completion(
             prompt_turns=prompt,
diff --git a/src/evals/prompts/continuation_prompt.py b/src/evals/prompts/continuation_prompt.py
index ec802ba..ad14e39 100644
--- a/src/evals/prompts/continuation_prompt.py
+++ b/src/evals/prompts/continuation_prompt.py
@@ -27,6 +27,7 @@ The sequences will be taken from the list of ambiguous sequences.
 # import random
 from typing import List, Union
 
+from src.models.openai_model import OpenAITextModels, OpenAIChatModels
 from src.evals.prompts.distribution_prompt import DISTRIBUTIONS
 from src.evals.utils import _generate_random_function, reformat_function
 
@@ -74,7 +75,7 @@ def create_continuation_prompt(
         text += ",".join([bin(x) for x in sequence])
     else:
         raise ValueError(f"Invalid base: {base}")
-    if model_name == "text-davinci-003":
+    if model_name in OpenAITextModels.list():
         # Prepend to the shots
         pretext = "Here are some examples of sequence continuations."
         pretext += "\n"
@@ -82,7 +83,7 @@ def create_continuation_prompt(
         text += "\n"
         text += "A: "
         return text
-    elif model_name == "gpt-3.5-turbo":
+    elif model_name in OpenAIChatModels.list():
         pretext = [
             {
                 "role": "system",
@@ -108,8 +109,8 @@ def generate_cont_shot_prompt(
         sequence = [eval(fn)(x) for x in range(sequence_length)]
     else:
         raise ValueError(f"Invalid shot method: {shot_method}")
-
-    if model_name == "text-davinci-003":
+    print("model name is: ", model_name)
+    if model_name in OpenAITextModels.list():
         text = "Q: "
         if base == 10:
             text += ",".join([str(x) for x in sequence])
@@ -123,7 +124,7 @@ def generate_cont_shot_prompt(
         text += "\n"
         return text
 
-    elif model_name == "gpt-3.5-turbo":
+    elif model_name in OpenAIChatModels.list():
         if base == 10:
             q_text = ",".join([str(x) for x in sequence])
             a_text = str(eval(fn)(sequence_length))
diff --git a/src/evals/prompts/explanation_prompt.py b/src/evals/prompts/explanation_prompt.py
index d6cca14..b38b062 100644
--- a/src/evals/prompts/explanation_prompt.py
+++ b/src/evals/prompts/explanation_prompt.py
@@ -28,7 +28,7 @@ from src.evals.prompts.distribution_prompt import DISTRIBUTIONS
 from src.evals.utils import _generate_random_function, reformat_function
 
 # from evals.utils import _generate_random_function, generate_wrong_functions
-from src.models.openai_model import DAVINCI_MODEL_NAME
+from src.models.openai_model import DAVINCI_MODEL_NAME, OpenAITextModels, OpenAIChatModels
 from src.pipelines.sequence_completions import sequence_functions
 
 PRE_PROMPT = """
@@ -53,7 +53,7 @@ def create_explanation_prompt(
     Create a prompt to continue a sequence of numbers.
     """
     sequence_length = len(sequence)
-    prompt_text = "" if model_name == "text-davinci-003" else []
+    prompt_text = "" if model_name in OpenAITextModels.list() else []
     if shots > 0:
         for i in range(shots):
             # Note: we are using the sequence length implicitly specified by
@@ -76,14 +76,14 @@ def create_explanation_prompt(
     pre_prompt = PRE_PROMPT
     pre_prompt = pre_prompt.format(base)
     # print(pre_prompt)
-    if model_name == "text-davinci-003":
+    if model_name in OpenAITextModels.list():
         # Prepend to the shots
         pretext = pre_prompt + "\n"
         pretext += "\n"
         text = pretext + prompt_text + text
         text += "\n"
         return text
-    elif model_name == "gpt-3.5-turbo":
+    elif model_name in OpenAIChatModels.list():
         pretext = [
             {
                 "role": "system",
@@ -111,7 +111,7 @@ def generate_exp_shot_prompt(
     else:
         raise ValueError(f"Invalid shot method: {shot_method}")
 
-    if model_name == "text-davinci-003":
+    if model_name in OpenAITextModels.list():
         text = "Q: "
         text += ",".join([str(x) for x in sequence])
         text += "\n"
@@ -121,7 +121,7 @@ def generate_exp_shot_prompt(
         text += "\n"
         return text
 
-    elif model_name == "gpt-3.5-turbo":
+    elif model_name in OpenAIChatModels.list():
         if base == 10:
             q_text = ",".join([str(x) for x in sequence])
         elif base == 2:
@@ -132,6 +132,7 @@ def generate_exp_shot_prompt(
         return response
 
     else:
+        print("model name is: ", model_name)
         raise ValueError(f"Invalid model name: {model_name}")
 
 
diff --git a/src/evals/sequence_completion_with_base_change.py b/src/evals/sequence_completion_with_base_change.py
index 24f2112..84b69d8 100644
--- a/src/evals/sequence_completion_with_base_change.py
+++ b/src/evals/sequence_completion_with_base_change.py
@@ -61,7 +61,7 @@ def evaluate_compute_dependence_with_base_changes(
                     )
                 except Exception as e:
                     print("oopies")
-                    print(e)
+                    raise e
                 else:
                     if sequence in results:
                         results[sequence][
diff --git a/src/models/openai_model.py b/src/models/openai_model.py
index 6c18f7e..2cb9245 100644
--- a/src/models/openai_model.py
+++ b/src/models/openai_model.py
@@ -6,6 +6,8 @@ from typing import List, Union
 
 import openai
 
+from src.models.utils import INVALID_RESPONSE, ExtendedEnum
+
 CHAT_PROMPT_TEMPLATE = {"role": "user", "content": ""}
 # TEXT_PROMPT_TEMPLATE is just a simple string or array of strings
 DAVINCI_MODEL_NAME = "text-davinci-003"
@@ -16,11 +18,11 @@ INVALID_RESPONSE = "INVALID_RESPONSE"
 openai.api_key = os.getenv("OPENAI_API_KEY")
 
 
-class OpenAITextModels(Enum):
+class OpenAITextModels(ExtendedEnum):
     TEXT_DAVINCI_003 = "text-davinci-003"
 
 
-class OpenAIChatModels(Enum):
+class OpenAIChatModels(ExtendedEnum):
     CHAT_GPT_35 = "gpt-3.5-turbo"
     CHAT_GPT_4 = "gpt-4-0314"
 
[2023-05-26 11:19:48,806][__main__][ERROR] - list index out of range
Traceback (most recent call last):
  File "/Users/olejorgensen/Documents/AISC/introspective-self-consistency/src/utils.py", line 58, in decorated
    return func(*args, **kwargs)
  File "/Users/olejorgensen/Documents/AISC/introspective-self-consistency/main.py", line 36, in main
    evaluate_compute_dependence_with_base_changes(
  File "/Users/olejorgensen/Documents/AISC/introspective-self-consistency/src/evals/sequence_completion_with_base_change.py", line 64, in evaluate_compute_dependence_with_base_changes
    raise e
  File "/Users/olejorgensen/Documents/AISC/introspective-self-consistency/src/evals/sequence_completion_with_base_change.py", line 52, in evaluate_compute_dependence_with_base_changes
    ) = self_consistency_evaluation(
  File "/Users/olejorgensen/Documents/AISC/introspective-self-consistency/src/evals/check_self_consistency.py", line 40, in self_consistency_evaluation
    continuation_prompt = create_continuation_prompt(
  File "/Users/olejorgensen/Documents/AISC/introspective-self-consistency/src/evals/prompts/continuation_prompt.py", line 63, in create_continuation_prompt
    shot_prompt = generate_cont_shot_prompt(
  File "/Users/olejorgensen/Documents/AISC/introspective-self-consistency/src/evals/prompts/continuation_prompt.py", line 109, in generate_cont_shot_prompt
    sequence = [eval(fn)(x) for x in range(sequence_length)]
  File "/Users/olejorgensen/Documents/AISC/introspective-self-consistency/src/evals/prompts/continuation_prompt.py", line 109, in <listcomp>
    sequence = [eval(fn)(x) for x in range(sequence_length)]
  File "<string>", line 1, in <lambda>
IndexError: list index out of range
