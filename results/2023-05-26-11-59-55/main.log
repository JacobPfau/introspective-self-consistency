[2023-05-26 11:59:55,394][src.utils][INFO] - Git sha: 801edc7a631335386ba3c9413a1e5c6be97303f7
[2023-05-26 11:59:55,411][src.utils][INFO] - Changed files: ['conf/tasks/compute_dependence_with_base_changes.yaml', 'src/evals/evaluate_continuation.py', 'src/evals/evaluate_explanation.py', 'src/evals/prompts/continuation_prompt.py', 'src/evals/prompts/distribution_prompt.py', 'src/evals/prompts/explanation_prompt.py', 'src/models/openai_model.py']
[2023-05-26 11:59:55,425][src.utils][INFO] - Git diff:
diff --git a/conf/tasks/compute_dependence_with_base_changes.yaml b/conf/tasks/compute_dependence_with_base_changes.yaml
index 29b202c..366c20f 100644
--- a/conf/tasks/compute_dependence_with_base_changes.yaml
+++ b/conf/tasks/compute_dependence_with_base_changes.yaml
@@ -3,5 +3,5 @@ model: gpt-3.5-turbo
 num_shots: 4
 on_ambiguous_sequences: true
 num_samples: 1
-distribution: forced-consistency
+distribution: default
 shot_method: random
diff --git a/src/evals/evaluate_continuation.py b/src/evals/evaluate_continuation.py
index 65788a6..e1f7f45 100644
--- a/src/evals/evaluate_continuation.py
+++ b/src/evals/evaluate_continuation.py
@@ -7,6 +7,8 @@ from src.models.openai_model import (
     generate_completion,
 )
 
+from src.models.openai_model import OpenAITextModels, OpenAIChatModels
+
 
 def valid_continuation(
     model_continuation: str,
@@ -23,6 +25,7 @@ def valid_continuation(
         if base == 10:
             int(model_continuation)
         elif base == 2:
+
             int(model_continuation[2:], 2)
     except ValueError:
         return False
@@ -38,7 +41,7 @@ def generate_continuation(
     """
     Given a prompt, generate a continuation from the model.
     """
-    if model_name == "text-davinci-003":
+    if model_name in OpenAITextModels.list():
         # Feed this into the model
         model_response = generate_completion(
             prompt=prompt,
@@ -46,7 +49,7 @@ def generate_continuation(
             max_tokens=256,
             model=DAVINCI_MODEL_NAME,
         )
-    elif model_name == "gpt-3.5-turbo":
+    elif model_name in OpenAIChatModels.list():
         # Feed this into the model
         model_response = generate_chat_completion(
             prompt_turns=prompt,
diff --git a/src/evals/evaluate_explanation.py b/src/evals/evaluate_explanation.py
index e1f12d4..ff3071f 100644
--- a/src/evals/evaluate_explanation.py
+++ b/src/evals/evaluate_explanation.py
@@ -7,6 +7,8 @@ from src.models.openai_model import (
     generate_completion,
 )
 
+from src.models.openai_model import OpenAITextModels, OpenAIChatModels
+
 
 def valid_explanation(
     fn_form: str,
@@ -55,7 +57,7 @@ def generate_explanation(
     Given a prompt, generate an explanation from the model.
     TODO: refactor code, entirely copied from generate_continuation
     """
-    if model_name == "text-davinci-003":
+    if model_name in OpenAITextModels.list():
         # Feed this into the model
         model_response = generate_completion(
             prompt=prompt,
@@ -63,7 +65,7 @@ def generate_explanation(
             max_tokens=256,
             model=DAVINCI_MODEL_NAME,
         )
-    elif model_name == "gpt-3.5-turbo":
+    elif model_name in OpenAIChatModels.list():
         # Feed this into the model
         model_response = generate_chat_completion(
             prompt_turns=prompt,
diff --git a/src/evals/prompts/continuation_prompt.py b/src/evals/prompts/continuation_prompt.py
index ec802ba..ad14e39 100644
--- a/src/evals/prompts/continuation_prompt.py
+++ b/src/evals/prompts/continuation_prompt.py
@@ -27,6 +27,7 @@ The sequences will be taken from the list of ambiguous sequences.
 # import random
 from typing import List, Union
 
+from src.models.openai_model import OpenAITextModels, OpenAIChatModels
 from src.evals.prompts.distribution_prompt import DISTRIBUTIONS
 from src.evals.utils import _generate_random_function, reformat_function
 
@@ -74,7 +75,7 @@ def create_continuation_prompt(
         text += ",".join([bin(x) for x in sequence])
     else:
         raise ValueError(f"Invalid base: {base}")
-    if model_name == "text-davinci-003":
+    if model_name in OpenAITextModels.list():
         # Prepend to the shots
         pretext = "Here are some examples of sequence continuations."
         pretext += "\n"
@@ -82,7 +83,7 @@ def create_continuation_prompt(
         text += "\n"
         text += "A: "
         return text
-    elif model_name == "gpt-3.5-turbo":
+    elif model_name in OpenAIChatModels.list():
         pretext = [
             {
                 "role": "system",
@@ -108,8 +109,8 @@ def generate_cont_shot_prompt(
         sequence = [eval(fn)(x) for x in range(sequence_length)]
     else:
         raise ValueError(f"Invalid shot method: {shot_method}")
-
-    if model_name == "text-davinci-003":
+    print("model name is: ", model_name)
+    if model_name in OpenAITextModels.list():
         text = "Q: "
         if base == 10:
             text += ",".join([str(x) for x in sequence])
@@ -123,7 +124,7 @@ def generate_cont_shot_prompt(
         text += "\n"
         return text
 
-    elif model_name == "gpt-3.5-turbo":
+    elif model_name in OpenAIChatModels.list():
         if base == 10:
             q_text = ",".join([str(x) for x in sequence])
             a_text = str(eval(fn)(sequence_length))
diff --git a/src/evals/prompts/distribution_prompt.py b/src/evals/prompts/distribution_prompt.py
index 026eb92..6e09f04 100644
--- a/src/evals/prompts/distribution_prompt.py
+++ b/src/evals/prompts/distribution_prompt.py
@@ -13,7 +13,7 @@ Give your response as a number, with no additional commentary.
 SELF_EXPLANATION_PROMPT = (
 """
 In your opinion, what is the most likely function which generated the following sequence?
-Assume the first number was generated by f(0), the second by f(1), and so on.
+Assume the first number was generated by f(0), the second by f(1), the third by f(2), and so on.
 The function should output the number in decimal form.
 Give your response in the format:
 Explanation: <function>
diff --git a/src/evals/prompts/explanation_prompt.py b/src/evals/prompts/explanation_prompt.py
index d6cca14..f6339cd 100644
--- a/src/evals/prompts/explanation_prompt.py
+++ b/src/evals/prompts/explanation_prompt.py
@@ -28,13 +28,13 @@ from src.evals.prompts.distribution_prompt import DISTRIBUTIONS
 from src.evals.utils import _generate_random_function, reformat_function
 
 # from evals.utils import _generate_random_function, generate_wrong_functions
-from src.models.openai_model import DAVINCI_MODEL_NAME
+from src.models.openai_model import DAVINCI_MODEL_NAME, OpenAITextModels, OpenAIChatModels
 from src.pipelines.sequence_completions import sequence_functions
 
 PRE_PROMPT = """
 Here are some examples of sequence explanations, i.e. python functions
 which generated the preceding sequences base {}. Assume the first number was generated by f(0),
-the second by f(1), and so on.
+the second by f(1), the third by f(2), and so on.
 """
 
 # TODO: fix generating functions to include recursive progressions, an ok fix for now.
@@ -53,7 +53,7 @@ def create_explanation_prompt(
     Create a prompt to continue a sequence of numbers.
     """
     sequence_length = len(sequence)
-    prompt_text = "" if model_name == "text-davinci-003" else []
+    prompt_text = "" if model_name in OpenAITextModels.list() else []
     if shots > 0:
         for i in range(shots):
             # Note: we are using the sequence length implicitly specified by
@@ -76,14 +76,14 @@ def create_explanation_prompt(
     pre_prompt = PRE_PROMPT
     pre_prompt = pre_prompt.format(base)
     # print(pre_prompt)
-    if model_name == "text-davinci-003":
+    if model_name in OpenAITextModels.list():
         # Prepend to the shots
         pretext = pre_prompt + "\n"
         pretext += "\n"
         text = pretext + prompt_text + text
         text += "\n"
         return text
-    elif model_name == "gpt-3.5-turbo":
+    elif model_name in OpenAIChatModels.list():
         pretext = [
             {
                 "role": "system",
@@ -111,7 +111,7 @@ def generate_exp_shot_prompt(
     else:
         raise ValueError(f"Invalid shot method: {shot_method}")
 
-    if model_name == "text-davinci-003":
+    if model_name in OpenAITextModels.list():
         text = "Q: "
         text += ",".join([str(x) for x in sequence])
         text += "\n"
@@ -121,7 +121,7 @@ def generate_exp_shot_prompt(
         text += "\n"
         return text
 
-    elif model_name == "gpt-3.5-turbo":
+    elif model_name in OpenAIChatModels.list():
         if base == 10:
             q_text = ",".join([str(x) for x in sequence])
         elif base == 2:
@@ -132,6 +132,7 @@ def generate_exp_shot_prompt(
         return response
 
     else:
+        print("model name is: ", model_name)
         raise ValueError(f"Invalid model name: {model_name}")
 
 
diff --git a/src/models/openai_model.py b/src/models/openai_model.py
index 6c18f7e..2cb9245 100644
--- a/src/models/openai_model.py
+++ b/src/models/openai_model.py
@@ -6,6 +6,8 @@ from typing import List, Union
 
 import openai
 
+from src.models.utils import INVALID_RESPONSE, ExtendedEnum
+
 CHAT_PROMPT_TEMPLATE = {"role": "user", "content": ""}
 # TEXT_PROMPT_TEMPLATE is just a simple string or array of strings
 DAVINCI_MODEL_NAME = "text-davinci-003"
@@ -16,11 +18,11 @@ INVALID_RESPONSE = "INVALID_RESPONSE"
 openai.api_key = os.getenv("OPENAI_API_KEY")
 
 
-class OpenAITextModels(Enum):
+class OpenAITextModels(ExtendedEnum):
     TEXT_DAVINCI_003 = "text-davinci-003"
 
 
-class OpenAIChatModels(Enum):
+class OpenAIChatModels(ExtendedEnum):
     CHAT_GPT_35 = "gpt-3.5-turbo"
     CHAT_GPT_4 = "gpt-4-0314"
 
