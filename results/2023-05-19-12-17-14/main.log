[2023-05-19 12:17:14,711][src.utils][INFO] - Git sha: 49285a9aabd6a59e8597f9002edef5c3e0046bc4
[2023-05-19 12:17:14,728][src.utils][INFO] - Changed files: ['conf/tasks/compute_dependence_with_base_changes.yaml', 'main.py', 'src/evals/prompts/continuation_prompt.py', 'src/evals/prompts/distribution_prompt.py', 'src/evals/prompts/distributions.py', 'src/evals/prompts/explanation_prompt.py', 'src/evals/sequence_completion_with_base_change.py']
[2023-05-19 12:17:14,738][src.utils][INFO] - Git diff:
diff --git a/conf/tasks/compute_dependence_with_base_changes.yaml b/conf/tasks/compute_dependence_with_base_changes.yaml
index 25bfb79..366c20f 100644
--- a/conf/tasks/compute_dependence_with_base_changes.yaml
+++ b/conf/tasks/compute_dependence_with_base_changes.yaml
@@ -3,3 +3,5 @@ model: gpt-3.5-turbo
 num_shots: 4
 on_ambiguous_sequences: true
 num_samples: 1
+distribution: default
+shot_method: random
diff --git a/main.py b/main.py
index a363731..b6ee3d3 100644
--- a/main.py
+++ b/main.py
@@ -39,6 +39,8 @@ def main(cfg: DictConfig) -> None:
             num_shots=task_cfg.num_shots,
             on_ambiguous_sequences=task_cfg.on_ambiguous_sequences,
             num_samples=task_cfg.num_samples,
+            distribution=task_cfg.distribution,
+            shot_method=task_cfg.shot_method,
         )


diff --git a/src/evals/prompts/continuation_prompt.py b/src/evals/prompts/continuation_prompt.py
index 348318e..ec802ba 100644
--- a/src/evals/prompts/continuation_prompt.py
+++ b/src/evals/prompts/continuation_prompt.py
@@ -17,25 +17,6 @@ Prompts will take the form:
     ***CONTINUATION_PROMPT***

     A:
---------------------------
-    For (text) explanations, this will be of the form:
---------------------------
-    Here are some examples of sequence explanations, i.e. python functions
-    which could have generated the preceding sequences. Assume the first
-    number was generated by f(0), the second by f(1), and so on.
-    Sequence: 2, 4, 6
-    Explanation: lambda x: 2*(x+1)
-
-    Sequence: 1, 2, 3, 4, 5
-    Explanation: lambda x: (x+1)
-
-    Sequence: 9, 16, 25, 36
-    Explanation: lambda x: (x+3)**2
-
-    ***EXPLANATION_PROMPT***
-
-    Explanation:
-

 --------------------------

@@ -46,7 +27,7 @@ The sequences will be taken from the list of ambiguous sequences.
 # import random
 from typing import List, Union

-from src.evals.prompts.distributions import DISTRIBUTIONS
+from src.evals.prompts.distribution_prompt import DISTRIBUTIONS
 from src.evals.utils import _generate_random_function, reformat_function

 # from evals.utils import _generate_random_function, generate_wrong_functions
diff --git a/src/evals/prompts/distribution_prompt.py b/src/evals/prompts/distribution_prompt.py
index 8571ffb..82878c9 100644
--- a/src/evals/prompts/distribution_prompt.py
+++ b/src/evals/prompts/distribution_prompt.py
@@ -1,12 +1,23 @@
 """
-Generate a prompt which asks the model to choose from some distribution of functions.
+TODO: For Q2.1, we can prompt the model to choose the most likely function from
+different distbutions. This should be easiest to do by adding to the DISTRIBUTIONS
+dictionary in this file.
 """

 SELF_CONTINUATION_PROMPT = "In your opinion, what is the most likely continuation of the following sequence? Give your response as a number, with no additional commentary."

-SELF_EXPLANATION_PROMPT = """In your opinion, what is the most likely function which generated the following sequence?
+SELF_EXPLANATION_PROMPT = (
+"""In your opinion, what is the most likely function which generated the following sequence?
 Assume the first number was generated by f(0), the second by f(1), and so on.
 The function should output a binary string prefixed by 0b, i.e. '0b1010'.
 Give your response in the format:
 Explanation: <function>
 """
+)
+
+DISTRIBUTIONS = {
+    "default": {
+        "continuation": SELF_CONTINUATION_PROMPT,
+        "explanation": SELF_EXPLANATION_PROMPT,
+    }
+}
diff --git a/src/evals/prompts/distributions.py b/src/evals/prompts/distributions.py
deleted file mode 100644
index 76aec08..0000000
--- a/src/evals/prompts/distributions.py
+++ /dev/null
@@ -1,14 +0,0 @@
-"""
-Create a dictionary of distributions we might use for prompting explanations / continuations.
-"""
-from src.evals.prompts.distribution_prompt import (
-    SELF_CONTINUATION_PROMPT,
-    SELF_EXPLANATION_PROMPT,
-)
-
-DISTRIBUTIONS = {
-    "default": {
-        "continuation": SELF_CONTINUATION_PROMPT,
-        "explanation": SELF_EXPLANATION_PROMPT,
-    }
-}
diff --git a/src/evals/prompts/explanation_prompt.py b/src/evals/prompts/explanation_prompt.py
index 40f9b98..996a568 100644
--- a/src/evals/prompts/explanation_prompt.py
+++ b/src/evals/prompts/explanation_prompt.py
@@ -24,7 +24,7 @@ Prompts will take the form:

 from typing import List, Union

-from src.evals.prompts.distributions import DISTRIBUTIONS
+from src.evals.prompts.distribution_prompt import DISTRIBUTIONS
 from src.evals.utils import _generate_random_function, reformat_function

 # from evals.utils import _generate_random_function, generate_wrong_functions
@@ -73,7 +73,6 @@ def create_explanation_prompt(
         text += ",".join([str(x) for x in sequence])
     elif base == 2:
         text += ",".join([bin(x) for x in sequence])
-    # print("siiii")
     pre_prompt = PRE_PROMPT
     pre_prompt = pre_prompt.format(base)
     # print(pre_prompt)
@@ -105,10 +104,8 @@ def generate_exp_shot_prompt(
     """
     if shot_method == "random":
         fn, offset = _generate_random_function(sequence_functions, (0, 7), (0, 7))
-        # print("og fn is", fn)
         # Reformat fn to replace every x after the first with x+offset
         fn = reformat_function(fn, offset)
-        # print(fn)
         sequence = [eval(fn)(x) for x in range(sequence_length)]
     else:
         raise ValueError(f"Invalid shot method: {shot_method}")
@@ -156,5 +153,4 @@ def parse_explanation(model_response: str) -> tuple[str, str]:
             # Saving the value based on the key
             if key == "Explanation":
                 x = value
-    # print(x)
     return x
diff --git a/src/evals/sequence_completion_with_base_change.py b/src/evals/sequence_completion_with_base_change.py
index 38a2f1f..73dbff2 100644
--- a/src/evals/sequence_completion_with_base_change.py
+++ b/src/evals/sequence_completion_with_base_change.py
@@ -12,6 +12,8 @@ def evaluate_compute_dependence_with_base_changes(
     num_shots: int,
     on_ambiguous_sequences: bool,
     num_samples: int,
+    distribution: str = "default",
+    shot_method: str = "random",
 ):
     total = 0
     if on_ambiguous_sequences:
@@ -49,10 +51,10 @@ def evaluate_compute_dependence_with_base_changes(
                     ) = self_consistency_evaluation(
                         model_name=model,
                         sequence=int_sequence,
-                        distribution="default",
+                        distribution=distribution,
                         base=base,
                         shots=num_shots,
-                        shot_method="random",
+                        shot_method=shot_method,
                         temperature=0.0,
                         samples=num_samples,
                     )
[2023-05-19 12:17:47,821][openai][INFO] - error_code=None error_message='That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 85a12304f6e5a34be1c5080c4a88ed5e in your message.)' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False
