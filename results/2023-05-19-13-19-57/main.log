[2023-05-19 13:19:57,353][src.utils][INFO] - Git sha: 49285a9aabd6a59e8597f9002edef5c3e0046bc4
[2023-05-19 13:19:57,365][src.utils][INFO] - Changed files: ['conf/tasks/compute_dependence_with_base_changes.yaml', 'main.py', 'src/evals/check_self_consistency.py', 'src/evals/prompts/continuation_prompt.py', 'src/evals/prompts/distribution_prompt.py', 'src/evals/prompts/distributions.py', 'src/evals/prompts/explanation_prompt.py', 'src/evals/sequence_completion_with_base_change.py']
[2023-05-19 13:19:57,374][src.utils][INFO] - Git diff:
diff --git a/conf/tasks/compute_dependence_with_base_changes.yaml b/conf/tasks/compute_dependence_with_base_changes.yaml
index 25bfb79..3e1ca35 100644
--- a/conf/tasks/compute_dependence_with_base_changes.yaml
+++ b/conf/tasks/compute_dependence_with_base_changes.yaml
@@ -1,5 +1,7 @@
-sequence_type: binary
+sequence_type: integer
 model: gpt-3.5-turbo
 num_shots: 4
 on_ambiguous_sequences: true
 num_samples: 1
+distribution: default
+shot_method: random
diff --git a/main.py b/main.py
index a363731..b6ee3d3 100644
--- a/main.py
+++ b/main.py
@@ -39,6 +39,8 @@ def main(cfg: DictConfig) -> None:
             num_shots=task_cfg.num_shots,
             on_ambiguous_sequences=task_cfg.on_ambiguous_sequences,
             num_samples=task_cfg.num_samples,
+            distribution=task_cfg.distribution,
+            shot_method=task_cfg.shot_method,
         )


diff --git a/src/evals/check_self_consistency.py b/src/evals/check_self_consistency.py
index 1634921..cbe9dbd 100644
--- a/src/evals/check_self_consistency.py
+++ b/src/evals/check_self_consistency.py
@@ -121,7 +121,7 @@ def self_consistency_evaluation(
             invalid_responses += 1
             continue

-        if continuation == int(implied_continuation):
+        if int(continuation) == int(implied_continuation):
             consistent = True
         else:
             consistent = False
diff --git a/src/evals/prompts/continuation_prompt.py b/src/evals/prompts/continuation_prompt.py
index 348318e..ec802ba 100644
--- a/src/evals/prompts/continuation_prompt.py
+++ b/src/evals/prompts/continuation_prompt.py
@@ -17,25 +17,6 @@ Prompts will take the form:
     ***CONTINUATION_PROMPT***

     A:
---------------------------
-    For (text) explanations, this will be of the form:
---------------------------
-    Here are some examples of sequence explanations, i.e. python functions
-    which could have generated the preceding sequences. Assume the first
-    number was generated by f(0), the second by f(1), and so on.
-    Sequence: 2, 4, 6
-    Explanation: lambda x: 2*(x+1)
-
-    Sequence: 1, 2, 3, 4, 5
-    Explanation: lambda x: (x+1)
-
-    Sequence: 9, 16, 25, 36
-    Explanation: lambda x: (x+3)**2
-
-    ***EXPLANATION_PROMPT***
-
-    Explanation:
-

 --------------------------

@@ -46,7 +27,7 @@ The sequences will be taken from the list of ambiguous sequences.
 # import random
 from typing import List, Union

-from src.evals.prompts.distributions import DISTRIBUTIONS
+from src.evals.prompts.distribution_prompt import DISTRIBUTIONS
 from src.evals.utils import _generate_random_function, reformat_function

 # from evals.utils import _generate_random_function, generate_wrong_functions
diff --git a/src/evals/prompts/distribution_prompt.py b/src/evals/prompts/distribution_prompt.py
index 8571ffb..e2b0378 100644
--- a/src/evals/prompts/distribution_prompt.py
+++ b/src/evals/prompts/distribution_prompt.py
@@ -1,12 +1,23 @@
 """
-Generate a prompt which asks the model to choose from some distribution of functions.
+TODO: For Q2.1, we can prompt the model to choose the most likely function from
+different distbutions. This should be easiest to do by adding to the DISTRIBUTIONS
+dictionary in this file.
 """

 SELF_CONTINUATION_PROMPT = "In your opinion, what is the most likely continuation of the following sequence? Give your response as a number, with no additional commentary."

-SELF_EXPLANATION_PROMPT = """In your opinion, what is the most likely function which generated the following sequence?
+SELF_EXPLANATION_PROMPT = (
+"""In your opinion, what is the most likely function which generated the following sequence?
 Assume the first number was generated by f(0), the second by f(1), and so on.
-The function should output a binary string prefixed by 0b, i.e. '0b1010'.
+The function should output the number in decimal form.
 Give your response in the format:
 Explanation: <function>
 """
+)
+
+DISTRIBUTIONS = {
+    "default": {
+        "continuation": SELF_CONTINUATION_PROMPT,
+        "explanation": SELF_EXPLANATION_PROMPT,
+    }
+}
diff --git a/src/evals/prompts/distributions.py b/src/evals/prompts/distributions.py
deleted file mode 100644
index 76aec08..0000000
--- a/src/evals/prompts/distributions.py
+++ /dev/null
@@ -1,14 +0,0 @@
-"""
-Create a dictionary of distributions we might use for prompting explanations / continuations.
-"""
-from src.evals.prompts.distribution_prompt import (
-    SELF_CONTINUATION_PROMPT,
-    SELF_EXPLANATION_PROMPT,
-)
-
-DISTRIBUTIONS = {
-    "default": {
-        "continuation": SELF_CONTINUATION_PROMPT,
-        "explanation": SELF_EXPLANATION_PROMPT,
-    }
-}
diff --git a/src/evals/prompts/explanation_prompt.py b/src/evals/prompts/explanation_prompt.py
index 40f9b98..d6cca14 100644
--- a/src/evals/prompts/explanation_prompt.py
+++ b/src/evals/prompts/explanation_prompt.py
@@ -24,7 +24,7 @@ Prompts will take the form:

 from typing import List, Union

-from src.evals.prompts.distributions import DISTRIBUTIONS
+from src.evals.prompts.distribution_prompt import DISTRIBUTIONS
 from src.evals.utils import _generate_random_function, reformat_function

 # from evals.utils import _generate_random_function, generate_wrong_functions
@@ -73,7 +73,6 @@ def create_explanation_prompt(
         text += ",".join([str(x) for x in sequence])
     elif base == 2:
         text += ",".join([bin(x) for x in sequence])
-    # print("siiii")
     pre_prompt = PRE_PROMPT
     pre_prompt = pre_prompt.format(base)
     # print(pre_prompt)
@@ -92,6 +91,7 @@ def create_explanation_prompt(
             }
         ]
         whole_prompt = pretext + prompt_text + [{"role": "user", "content": text}]
+        print(whole_prompt)
         return whole_prompt
     else:
         raise ValueError(f"Invalid model name: {model_name}")
@@ -105,10 +105,8 @@ def generate_exp_shot_prompt(
     """
     if shot_method == "random":
         fn, offset = _generate_random_function(sequence_functions, (0, 7), (0, 7))
-        # print("og fn is", fn)
         # Reformat fn to replace every x after the first with x+offset
         fn = reformat_function(fn, offset)
-        # print(fn)
         sequence = [eval(fn)(x) for x in range(sequence_length)]
     else:
         raise ValueError(f"Invalid shot method: {shot_method}")
@@ -156,5 +154,4 @@ def parse_explanation(model_response: str) -> tuple[str, str]:
             # Saving the value based on the key
             if key == "Explanation":
                 x = value
-    # print(x)
     return x
diff --git a/src/evals/sequence_completion_with_base_change.py b/src/evals/sequence_completion_with_base_change.py
index 38a2f1f..24f2112 100644
--- a/src/evals/sequence_completion_with_base_change.py
+++ b/src/evals/sequence_completion_with_base_change.py
@@ -12,6 +12,8 @@ def evaluate_compute_dependence_with_base_changes(
     num_shots: int,
     on_ambiguous_sequences: bool,
     num_samples: int,
+    distribution: str = "default",
+    shot_method: str = "random",
 ):
     total = 0
     if on_ambiguous_sequences:
@@ -40,6 +42,7 @@ def evaluate_compute_dependence_with_base_changes(
             print(f"Sequence: {sequence}")
             for _ in range(2):
                 try:
+                    print("base be: ", base)
                     (
                         correct_consistent_explanations,
                         correct_inconsistent_explanations,
@@ -49,10 +52,10 @@ def evaluate_compute_dependence_with_base_changes(
                     ) = self_consistency_evaluation(
                         model_name=model,
                         sequence=int_sequence,
-                        distribution="default",
+                        distribution=distribution,
                         base=base,
                         shots=num_shots,
-                        shot_method="random",
+                        shot_method=shot_method,
                         temperature=0.0,
                         samples=num_samples,
                     )
